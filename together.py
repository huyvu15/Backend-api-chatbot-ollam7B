import os
import glob
import PyPDF2
import requests
from typing import List, Dict, Any
from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_huggingface import HuggingFaceEmbeddings  # C·∫≠p nh·∫≠t import
from langchain.embeddings import HuggingFaceEmbeddings

from langchain_community.vectorstores import FAISS  # C·∫≠p nh·∫≠t import
from langchain.docstore.document import Document


def load_pdfs(folder_path: str) -> List[Dict[str, Any]]:
    pdf_docs = []
    unique_files = set()
    for file in glob.glob(os.path.join(folder_path, "*.pdf")):
        try:
            if file in unique_files:
                continue
            unique_files.add(file)
            
            with open(file, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
                pdf_docs.append({
                    "text": text,
                    "file_path": file
                })
        except Exception as e:
            print(f"L·ªói x·ª≠ l√Ω t·ªáp {file}: {e}")
    return pdf_docs


def chunk_documents(pdf_docs: List[Dict[str, Any]], chunk_size=500, chunk_overlap=100) -> List[Document]:
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", " ", ""]
    )
    
    documents = []
    for doc in pdf_docs:
        chunks = text_splitter.split_text(doc["text"])
        for chunk in chunks:
            documents.append(Document(
                page_content=chunk, 
                metadata={
                    "file_path": doc["file_path"], 
                    "full_text": doc["text"]
                }
            ))
    return documents


def create_vectorstore(documents: List[Document], embedding_model: HuggingFaceEmbeddings) -> FAISS:
    return FAISS.from_documents(documents, embedding_model)


def search_candidates(query: str, vectorstore: FAISS, top_k: int = 5) -> List[Dict[str, str]]:
    retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
    results = retriever.get_relevant_documents(query)
    
    unique_results = []
    seen_paths = set()
    for doc in results:
        if doc.metadata["file_path"] not in seen_paths:
            unique_results.append({
                "file_path": doc.metadata["file_path"], 
                "context": doc.metadata["full_text"]
            })
            seen_paths.add(doc.metadata["file_path"])
    
    return unique_results


def generate_response(query: str, context: str, together_api_key: str, together_model: str) -> str:
    if not together_api_key:
        raise ValueError("Kh√≥a API Together.ai l√† b·∫Øt bu·ªôc")
    
    headers = {
        "Authorization": f"Bearer {together_api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": together_model,
        "prompt": f"""B·∫°n l√† m·ªôt chatbot AI c√≥ kh·∫£ nƒÉng t√¨m ki·∫øm v√† ƒë∆∞a ra th√¥ng tin c·ªßa ·ª©ng vi√™n ph√π h·ª£p, d·ª±a tr√™n to√†n b·ªô b·ªëi c·∫£nh CV ·ª©ng vi√™n ƒë∆∞·ª£c t√¨m th·∫•y sau ƒë√¢y, h√£y ƒë∆∞a ra ph√¢n t√≠ch v√† tr·∫£ l·ªùi √Ω ki·∫øn c·ªßa b·∫°n c√¢u truy v·∫•n m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c:
    
B·ªëi c·∫£nh CV: {context}
C√¢u truy v·∫•n: {query}

Tr·∫£ l·ªùi chi ti·∫øt b·∫±ng ti·∫øng Vi·ªát:""",
        "max_tokens": 500,
        "temperature": 0.7
    }
    
    try:
        response = requests.post(
            "https://api.together.xyz/v1/completions", 
            headers=headers, 
            json=payload
        )
        
        response_data = response.json()
        if 'choices' in response_data and len(response_data['choices']) > 0:
            return response_data['choices'][0]['text'].strip()
        elif 'output' in response_data:
            return response_data['output'].strip()
        else:
            return "Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p v·ªõi truy v·∫•n."
    
    except requests.RequestException as e:
        return f"L·ªói k·∫øt n·ªëi: {e}"
    except Exception as e:
        return f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}"


def process_search(query: str, vectorstore: FAISS, together_api_key: str, together_model: str) -> List[Dict[str, str]]:
    matched_candidates = search_candidates(query, vectorstore)
    
    results = []
    for candidate in matched_candidates:
        try:
            response = generate_response(query, candidate['context'], together_api_key, together_model)
            
            results.append({
                "file_path": candidate['file_path'],
                "response": response + f"\n\nüîó ƒê∆∞·ªùng d·∫´n CV: {candidate['file_path']}"
            })
        except Exception as e:
            print(f"L·ªói x·ª≠ l√Ω t·ªáp {candidate['file_path']}: {e}")
    
    return results


def main():
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2", model_kwargs={"device": "cpu"}, encode_kwargs={"normalize_embeddings": True})
    together_api_key = "eca0b727abc5861fdcb4ea8bfcad9e1c165fd552cf1b70859350cad33ba8e15d"
    together_model = "meta-llama/Llama-2-7b-chat-hf"
    folder_path = "D:\\Project2\\data\\test"
    
    pdf_docs = load_pdfs(folder_path)
    documents = chunk_documents(pdf_docs)
    
    vectorstore = create_vectorstore(documents, embedding_model)
    
    while True:
        query = input("\nNh·∫≠p c√¢u truy v·∫•n (ho·∫∑c 'exit'): ")
        
        if query.lower() == 'exit':
            print("K·∫øt th√∫c ch∆∞∆°ng tr√¨nh.")
            break
        
        results = process_search(query, vectorstore, together_api_key, together_model)
        
        if results:
            for result in results:
                print(f"\nPh·∫£n h·ªìi: {result['response']}\n")
        else:
            print("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ ph√π h·ª£p.")


if __name__ == "__main__":
    main()
